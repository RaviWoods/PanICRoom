{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5b2497b3-60ee-7cd0-0625-f103214c0ed4"
   },
   "source": [
    "A piece of text usually conveys an author's attitude towards a certain topic. This can be positive, negative, or neutral (e.g. Reviews on Amazon, IMDB, RottenTomatoes, etc..). Sentiment Analysis tries to infer the *sentiment* via computational modelling of text. \n",
    "\n",
    "- In this tutorial, we will build a simple LSTM model to perform sentiment classification.\n",
    "\n",
    "(Note that this tutorial is largely inspired by https://www.kaggle.com/ngyptr/lstm-sentiment-analysis-keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "39c44f0e-d62c-7e11-a542-4fcd58e21442"
   },
   "source": [
    "#### Load necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might need to install pandas (data/table processing library) before starting this tutorial\n",
    "> pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "6c53202d-5c34-4859-e7e9-8ef5c7068287"
   },
   "outputs": [],
   "source": [
    "# This is a regex library. Regex stands for (reg)ular (ex)pression. \n",
    "# This lets you manipulate string in a very smart&convenient way.\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "6c53202d-5c34-4859-e7e9-8ef5c7068287"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Data processing libraries\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "# Libraries for text mipulation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Libraries for building deep learning model. \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "\n",
    "# Libraries to perform standard machine learing training\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2bc2702e-d6f4-df5f-b80e-50ab23a6d29e"
   },
   "source": [
    "### Data Loading and Visualisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this exercise, we will use dataset which contains tweets about First GOP Debate 2016. \n",
    "\n",
    "- The original source says: *We looked through tens of thousands of tweets about the early August GOP debate in Ohio and asked contributors to do both sentiment analysis and data categorization. Contributors were asked if the tweet was **relevant**, which **candidate** was mentioned, what **subject** was mentioned, and then what the **sentiment** was for a given tweet. We've removed the non-relevant messages from the uploaded dataset.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "89c8c923-c0bf-7b35-9ab8-e63f00b74e5a"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'Sentiment.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-893447882c7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Sentiment.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/vol/medic01/users/cq615/packages/anaconda3/envs/deeplearning/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/vol/medic01/users/cq615/packages/anaconda3/envs/deeplearning/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/vol/medic01/users/cq615/packages/anaconda3/envs/deeplearning/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/vol/medic01/users/cq615/packages/anaconda3/envs/deeplearning/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/vol/medic01/users/cq615/packages/anaconda3/envs/deeplearning/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'Sentiment.csv' does not exist"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('Sentiment.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's familiarise ourselves with data! \n",
    "Sentiment.csv is a dataset in the form of a table, where:\n",
    "- Each row corresponds to one item (tweet1, tweet2, ...) \n",
    "- Each column corresponds to specific attribute ('tweet_id', 'tweet_name', 'subject', 'sentiment', etc..) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of entries, and the number of fields per entry\n",
    "n_entries, n_fields = data.shape\n",
    "\n",
    "print('The dataset has {} entries and each entry has {} fields'.format(n_entries, n_fields))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view first entry to see how the data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pandas, it is easy to extract the only entries that you care about, as follows. The following code first extracts the relevant *columns*, then views first 5 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['candidate','name', 'text', 'sentiment',]].loc[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how entry 4's *text* field looks like in detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis via LSTM Classifier\n",
    "\n",
    "Let's infer sentiment of texts using Long-Short-Term-Memory (LSTM). We will:\n",
    "1. Preprocessing the data, then prepare the input and output for the network\n",
    "2. Build a network model for classification (model specification)\n",
    "3. Train the network! \n",
    "4. Evaluate the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1: Preprocess/Data Cleaning**: \n",
    "The first stage is to clean data in a way that the neural network can handle data. \n",
    "- We will perform a binary classification of sentiment: whether the text conveys positive or negative sentiment. Therefore, \n",
    "    - (1) We will extract 2 fields we care about: *text* and *sentiment*. \n",
    "    - (2) We will delete the neutral sentiments from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['text', 'sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.sentiment != \"Neutral\"]\n",
    "print(\"Number of tweets with positive sentiment: {}\".format(data[ data['sentiment'] == 'Positive'].size))\n",
    "print(\"Number of tweets with negative sentiment: {}\".format(data[ data['sentiment'] == 'Negative'].size))\n",
    "print(\"Number of tweets with neutral sentiment: {}\".format(data[ data['sentiment'] == 'Neutral'].size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- At the moment, the text field contains a lot of symbols that is not actually part of the message. Also, we don't want to worry too much about case sensitivity. The following code cleans that up abit using **regex** (Don't worry too much about this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: x.lower())\n",
    "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "data['text'] = data['text'].apply(lambda x: x.replace('rt', ' '))\n",
    "\n",
    "# Let's see how the text got converted\n",
    "data['text'][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the text looks better: it just contains all lower case words. There are still random stuff like \"httptco\" and \"w\". It would be very difficult to clean 20000 entries perfectly, so we will decide to live with this. When it affects the network's performance too much, we might come back to clean data further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Tokenization\n",
    "- Human can see word and understand it, but machines cannot. Everything is just a bunch of numbers to them. Therefore, we need to convert words into a convenient digital representation.\n",
    "- For example, if I want to represent 26 alphabets (a, b, c, ..., x, y, z), we may use the representation (1, 2, 3, ...24, 25, 26) where the correspondance is: (a->1, b->2,...z->26)\n",
    "- For more general words in dictionary, we need more numbers. That's the idea of tokenization. In the following, we will decide the maximum number of features to be 2000. \n",
    "\n",
    "- We will then convert the text into a vectorised form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "43632d2d-6160-12ce-48b0-e5eb1c207076"
   },
   "outputs": [],
   "source": [
    "max_fatures = 2000\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(data['text'].values)\n",
    "X = tokenizer.texts_to_sequences(data['text'].values)\n",
    "\n",
    "# Let's see how the text is transformed into the representation that machine sees!\n",
    "print(X[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assigned a number to each word! \n",
    "\n",
    "Now, each tweet has different length. It is slightly easier for the network to handle input of the same length. We will \"pad\" the sequence so all tweets have the same number of length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pad_sequences(X)\n",
    "\n",
    "# Let's see how the text is transformed into the representation that machine sees!\n",
    "print(X[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will prepare the output of the network. At the moment, the target variables are 'Positive' and 'Negative'. Again, for the machines, we need a digital representation. We will Positive as [1, 0] and Negative as [0, 1]. This is called *one-hot encoding*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.get_dummies(data['sentiment']).values\n",
    "\n",
    "# Lets see how the output looks like:\n",
    "print(data['sentiment'][4], ' -> ', Y[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our input and output are ready!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2: Building a network\n",
    "\n",
    "Let's build a simple one-layer LSTM network for sentiment classification. We will be using Keras. \n",
    "\n",
    "Our network is composed of 3 components: \n",
    "- **Embedding layer**: The input we prepared above is just a sequence of numbers. This can be processed by the network, but it is not the best representation. For example, \"love\" and \"like\" are words that are similar. We want to be able to represent this \"similarity\" and \"disimilarity\" -- that's precisely the purpose of an embedding layer. In embedding layer, we map the above integers into a *continuous space* where we can define such notion of similarity as a distance metric. This space can have number of dimensions. This is your first hyper-parameter **embed_dim**.\n",
    "- **LSTM layer**: Given the input from embedding layer, we apply LSTM layer to learn hidden representation. hyper-parameter **lstm_out** is the number of hidden units we want to use. Larger the number, more expressive the network gets, but it also becomes more difficult to train this many hidden units.\n",
    "- **Classification layer**: Finally, given the hidden representation computed from the LSTM layer, we perform classification. This takes *lstm_out* number of inputs and map it into binary answer (positive/negative sentiment) via a fully connected layer called *Dense Layer*.\n",
    "\n",
    "Other two hyperparameters are:\n",
    "\n",
    "- **batch_size**: this determines how many number of data point we want to process at the same time. When the batch size is small, the network only learns from specific examples. If the batchsize is large, the network learns from a lot of words simultaneously.  \n",
    "- **droupout_x**: this parameter \"corrupts\" the network randomly. The higher this parameter, more corruption is introduced. By making the network learn while being corrupted, we make the network more robust. \n",
    "\n",
    "These hyperparameters needs to be adjusted to achieve the best performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1ba3cf60-a83c-9c21-05e0-b14303027e93"
   },
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "batchsize = 32\n",
    "dropout_x = 0.2\n",
    "\n",
    "# Build LSTM classifier with 3 components\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
    "model.add(LSTM(lstm_out, dropout=dropout_x, recurrent_dropout=dropout_x))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "15f4ee61-47e4-88c4-4b81-98a85237333f"
   },
   "source": [
    "### Step3: Train the network\n",
    "\n",
    "- From the dataset, we will split into training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b35748b8-2353-3db2-e571-5fd22bb93eb0"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
    "\n",
    "print('Size of training data: {}'.format(len(X_train)))\n",
    "print('Size of testing data: {}'.format(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2a775979-a930-e627-2963-18557d7bf6e6"
   },
   "source": [
    "- Now we will train the network! Since we don't have too much time, we will train for 5 epochs. Feel free to train for longer :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d5e499ac-2eba-6ff7-8d9a-ff65eb04099b"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "model.fit(X_train, Y_train, epochs = 5, batch_size=batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4ebd7bc1-53c0-0e31-a0b0-b6d0a3017434"
   },
   "source": [
    "### Step4: Evaluation \n",
    "We will measure the accuracy on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a970f412-722f-6d6d-72c8-325d0901ccef"
   },
   "outputs": [],
   "source": [
    "# Measure the overall accuracy on test data\n",
    "_, acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "print(\"Overall Accuracy: %.2f\" % (acc))\n",
    "\n",
    "# We will measure the accuracy on positive sentiment class and negative sentiment class\n",
    "pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\n",
    "for x in range(len(X_test)):\n",
    "    \n",
    "    result = model.predict(X_test[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "   \n",
    "    if np.argmax(result) == np.argmax(Y_test[x]):\n",
    "        if np.argmax(Y_test[x]) == 0:\n",
    "            neg_correct += 1\n",
    "        else:\n",
    "            pos_correct += 1\n",
    "       \n",
    "    if np.argmax(Y_test[x]) == 0:\n",
    "        neg_cnt += 1\n",
    "    else:\n",
    "        pos_cnt += 1\n",
    "\n",
    "print(\"Accuracy on Positive Sentiment: \", pos_correct/pos_cnt*100, \"%\")\n",
    "print(\"Accuracy on Negative Sentiment: \", neg_correct/neg_cnt*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "018ebf39-9414-27d0-232c-a34de051feaf"
   },
   "source": [
    "### Step 5: Enjoy!\n",
    "\n",
    "Now, define your own tweet and see what the network thinks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "24c64f46-edd1-8d0b-7c7c-ef50fd26b2fd"
   },
   "outputs": [],
   "source": [
    "twt = 'Meetings: Because none of us is as dumb as all of us'\n",
    "\n",
    "# Prepare the input: remember, we need preprocess the data: tokenization and padding. \n",
    "twt = tokenizer.texts_to_sequences(twt)\n",
    "twt = pad_sequences(twt, maxlen=28, dtype='int32', padding='post', truncating='post', value=0)\n",
    "\n",
    "# Feed the input to the network\n",
    "sentiment = model.predict(twt,batch_size=1,verbose = 2)[0]\n",
    "\n",
    "# What does network think?\n",
    "if(np.argmax(sentiment) == 0):\n",
    "    print(\"negative\")\n",
    "elif (np.argmax(sentiment) == 1):\n",
    "    print(\"positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Optimise the network\n",
    "\n",
    "Try adjusting 4 hyper-parameters mentioned: \n",
    "- batchsize \n",
    "- dropout_x \n",
    "- embed_dim \n",
    "- lstm_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "1. Why is the accuracy for positive sentiment so much lower than the negative one? There are no true answer to this question. Come up with as many reasons as you can. (Hint: for example, look at the number of examples for positive/negative sentiment. Can this be one of the reasons?)\n",
    "2. How did the hyper-parameters affected the performance? How would you improve the network performance further?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_change_revision": 185,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
